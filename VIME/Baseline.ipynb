{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from parser import get_parser\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Ndata(path='../Custom-Semi-Supervised/data/ndata.csv')\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(args=\n",
    "                         [\"--data\",\"real-n\", \n",
    "                          \"--sampling\",\"xgb\",\n",
    "                          \"--train_from\",\"20140101\",\n",
    "                          \"--test_from\",\"20170101\",\n",
    "                          \"--test_length\",\"365\",\n",
    "                          \"--valid_length\",\"90\",\n",
    "                          \"--initial_inspection_rate\", \"5\",\n",
    "                          \"--final_inspection_rate\", \"10\",\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:\n",
      "Train labeled: (54134, 52), Train unlabeled: (1028538, 52), Valid labeled: (70917, 52), Valid unlabeled: (0, 26), Test: (274808, 52)\n",
      "Checking label distribution\n",
      "Training: 0.05022795615481618\n",
      "Validation: 0.035556788645191434\n",
      "Testing: 0.025360899366070794\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "seed = args.seed\n",
    "epochs = args.epoch\n",
    "dim = args.dim\n",
    "lr = args.lr\n",
    "weight_decay = args.l2\n",
    "initial_inspection_rate = args.initial_inspection_rate\n",
    "inspection_rate_option = args.inspection_plan\n",
    "train_begin = args.train_from \n",
    "test_begin = args.test_from\n",
    "test_length = args.test_length\n",
    "valid_length = args.valid_length\n",
    "chosen_data = args.data\n",
    "numWeeks = args.numweeks\n",
    "semi_supervised = args.semi_supervised\n",
    "save = args.save\n",
    "gpu_id = args.device\n",
    "\n",
    "# Initial dataset split\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Initial dataset split\n",
    "train_start_day = datetime.date(int(train_begin[:4]), int(train_begin[4:6]), int(train_begin[6:8]))\n",
    "test_start_day = datetime.date(int(test_begin[:4]), int(test_begin[4:6]), int(test_begin[6:8]))\n",
    "test_length = timedelta(days=test_length)    \n",
    "test_end_day = test_start_day + test_length\n",
    "valid_length = timedelta(days=valid_length)\n",
    "valid_start_day = test_start_day - valid_length\n",
    "\n",
    "# data\n",
    "data.split(train_start_day, valid_start_day, test_start_day, test_end_day, valid_length, test_length, args)\n",
    "data.featureEngineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 2749\n",
      "Precision: 0.1550, Recall: 0.0627, Revenue: 0.1090\n",
      "Checking top 2% suspicious transactions: 5497\n",
      "Precision: 0.0906, Recall: 0.0733, Revenue: 0.1344\n",
      "Checking top 5% suspicious transactions: 13741\n",
      "Precision: 0.0463, Recall: 0.0936, Revenue: 0.1792\n",
      "Checking top 10% suspicious transactions: 27480\n",
      "Precision: 0.0462, Recall: 0.1867, Revenue: 0.2948\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from utils import *\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=4, n_jobs=-1, eval_metric=\"error\", scale_pos_weight = 1)\n",
    "xgb.fit(data.dftrainx_lab,data.train_cls_label)\n",
    "\n",
    "best_thresh, best_auc = find_best_threshold(xgb,data.dfvalidx_lab, data.valid_cls_label)\n",
    "xgb_test_pred = xgb.predict_proba(data.dftestx)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.test_cls_label,data.test_reg_label,args,best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 2749\n",
      "Precision: 0.1510, Recall: 0.0611, Revenue: 0.0912\n",
      "Checking top 2% suspicious transactions: 5497\n",
      "Precision: 0.0941, Recall: 0.0761, Revenue: 0.1321\n",
      "Checking top 5% suspicious transactions: 13739\n",
      "Precision: 0.0646, Recall: 0.1305, Revenue: 0.1884\n",
      "Checking top 10% suspicious transactions: 27481\n",
      "Precision: 0.1071, Recall: 0.4328, Revenue: 0.3864\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "X_train_leaves = xgb.apply(data.dftrainx_lab).reshape(-1,100)\n",
    "X_valid_leaves = xgb.apply(data.dfvalidx_lab).reshape(-1,100)\n",
    "X_test_leaves = xgb.apply(data.dftestx).reshape(-1,100)\n",
    "\n",
    "# One-hot encoding for leaf index\n",
    "xgbenc = OneHotEncoder(categories=\"auto\")\n",
    "lr_trainx = xgbenc.fit_transform(X_train_leaves)\n",
    "lr_validx = xgbenc.transform(X_valid_leaves)\n",
    "lr_testx = xgbenc.transform(X_test_leaves)\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(lr_trainx, data.train_cls_label)\n",
    "\n",
    "best_thresh, best_auc = find_best_threshold(lr,lr_validx, data.valid_cls_label)\n",
    "xgb_test_pred = lr.predict_proba(lr_testx)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.test_cls_label,data.test_reg_label,args,best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vime_self import vime_self\n",
    "from vime_semi import vime_semi\n",
    "from vime_utils import perf_metric\n",
    "from supervised_models import logit, xgb_model, mlp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental parameters\n",
    "label_no = 1000  \n",
    "model_sets = ['logit','xgboost','mlp']\n",
    "  \n",
    "# Hyper-parameters\n",
    "p_m = 0.3\n",
    "alpha = 2.0\n",
    "K = 3\n",
    "beta = 1.0\n",
    "label_data_rate = 0.1\n",
    "\n",
    "# Metric\n",
    "metric = 'acc'\n",
    "  \n",
    "# Define output\n",
    "results = np.zeros([len(model_sets)+2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "mlp_parameters = dict()\n",
    "mlp_parameters['hidden_dim'] = 100\n",
    "mlp_parameters['epochs'] = 100\n",
    "mlp_parameters['activation'] = 'relu'\n",
    "mlp_parameters['batch_size'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VIME-Semi\n",
    "vime_semi_parameters = dict()\n",
    "vime_semi_parameters['hidden_dim'] = 256\n",
    "vime_semi_parameters['batch_size'] = 128\n",
    "vime_semi_parameters['iterations'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "x_train = data.dftrainx_lab.values\n",
    "x_train = MinMaxScaler().fit_transform(x_train)\n",
    "y_train = to_categorical(data.train_cls_label)\n",
    "\n",
    "# unlab\n",
    "x_unlab = data.dftrainx_unlab.values\n",
    "x_unlab = MinMaxScaler().fit_transform(x_unlab)\n",
    "\n",
    "# test\n",
    "x_test = data.dftestx.values\n",
    "x_test = MinMaxScaler().fit_transform(x_test)\n",
    "y_test = to_categorical(data.test_cls_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1028538 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-25 19:17:24.287433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-01-25 19:17:24.316925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-25 19:17:24.318509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: NVIDIA GeForce RTX 2070 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.83\n",
      "pciBusID: 0000:01:00.0\n",
      "2022-01-25 19:17:24.318754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-25 19:17:24.320219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7465\n",
      "pciBusID: 0000:02:00.0\n",
      "2022-01-25 19:17:24.320975: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-01-25 19:17:24.539117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2022-01-25 19:17:24.539754: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-01-25 19:17:24.540034: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-01-25 19:17:24.540528: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-01-25 19:17:24.540783: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\n",
      "2022-01-25 19:17:25.059127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-01-25 19:17:25.059188: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-01-25 19:17:25.063352: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-01-25 19:17:25.204785: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3199980000 Hz\n",
      "2022-01-25 19:17:25.212577: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d820dc3580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-25 19:17:25.212645: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-01-25 19:17:25.463803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-25 19:17:25.480992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-25 19:17:25.483099: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d824fcb900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-25 19:17:25.483169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 SUPER, Compute Capability 7.5\n",
      "2022-01-25 19:17:25.483191: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1\n",
      "2022-01-25 19:17:25.483627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-01-25 19:17:25.483661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1028538/1028538 [==============================] - 37s 36us/sample - loss: 0.4249 - mask_loss: 0.3833 - feature_loss: 0.0208\n",
      "Epoch 2/50\n",
      "1028538/1028538 [==============================] - 36s 35us/sample - loss: 0.4116 - mask_loss: 0.3709 - feature_loss: 0.020430s - loss: 0.4144 - mask_loss: 0 - ETA: 2s - loss: 0.4118 - mask_loss: 0 - ETA: \n",
      "Epoch 3/50\n",
      "1028538/1028538 [==============================] - 36s 35us/sample - loss: 0.4075 - mask_loss: 0.3666 - feature_loss: 0.0205\n",
      "Epoch 4/50\n",
      "1028538/1028538 [==============================] - 36s 35us/sample - loss: 0.4075 - mask_loss: 0.3664 - feature_loss: 0.0205\n",
      "Epoch 5/50\n",
      "1028538/1028538 [==============================] - 36s 35us/sample - loss: 0.4082 - mask_loss: 0.3669 - feature_loss: 0.020616s - loss: 0.4080 - mask_loss: 0.366 - ETA: 15s - loss: 0.4080 - mask_loss: 0.3667 - fea - ETA: 14s - loss: 0.4081 - mask - ETA: 12s - loss: 0.4080 - mask_loss: 0.3\n",
      "Epoch 6/50\n",
      " 926704/1028538 [==========================>...] - ETA: 3s - loss: 0.4092 - mask_loss: 0.3677 - feature_loss: 0.0207 ETA: 4s - loss: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roytsai/.pyenv/versions/vime/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1028538/1028538 [==============================] - 36s 35us/sample - loss: 0.4092 - mask_loss: 0.3677 - feature_loss: 0.0207\n",
      "Epoch 7/50\n",
      " 638864/1028538 [=================>............] - ETA: 13s - loss: 0.4101 - mask_loss: 0.3685 - feature_loss: 0.0208"
     ]
    }
   ],
   "source": [
    "# # Train VIME-Self\n",
    "vime_self_parameters = dict()\n",
    "vime_self_parameters['batch_size'] = 16\n",
    "vime_self_parameters['epochs'] = 50\n",
    "vime_self_encoder = vime_self(x_unlab, p_m, alpha, vime_self_parameters)\n",
    "  \n",
    "# Save encoder\n",
    "if not os.path.exists('save_model'):\n",
    "    os.makedirs('save_model')\n",
    "\n",
    "file_name = './save_model/vime_Ndata2.h5'\n",
    "  \n",
    "vime_self_encoder.save(file_name)  \n",
    "        \n",
    "# Test VIME-Self\n",
    "x_train_hat = vime_self_encoder.predict(x_train)\n",
    "x_test_hat = vime_self_encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "results[3] = perf_metric(metric, y_test, y_test_hat)\n",
    "    \n",
    "print('VIME-Self Performance: ' + str(results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VIME-Semi\n",
    "vime_semi_parameters = dict()\n",
    "vime_semi_parameters['hidden_dim'] = 512\n",
    "vime_semi_parameters['batch_size'] = 32\n",
    "vime_semi_parameters['iterations'] = 1000\n",
    "y_test_hat = vime_semi(x_train, y_train, x_unlab, x_test, \n",
    "                       vime_semi_parameters, p_m, K, beta, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_f1,auc,pr, re, f, rev = metrics(y_test_hat[:,1], data.test_cls_label,data.test_reg_label,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vime",
   "language": "python",
   "name": "vime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
