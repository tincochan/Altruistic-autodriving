{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from parser import get_parser\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.utils import from_networkx, to_undirected\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Tdata(path='./data/tdata.csv')\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(args=\n",
    "                         [\"--data\",\"real-t\", \n",
    "                          \"--sampling\",\"xgb\",\n",
    "                          \"--mode\",\"scratch\",\n",
    "                          \"--train_from\",\"20170101\",\n",
    "                          \"--test_from\",\"20190101\",\n",
    "                          \"--test_length\",\"365\",\n",
    "                          \"--valid_length\",\"90\",\n",
    "                          \"--initial_inspection_rate\", \"2\",\n",
    "                          \"--final_inspection_rate\", \"10\",\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:\n",
      "Train labeled: (30957, 41), Train unlabeled: (1516868, 41), Valid labeled: (134457, 41), Valid unlabeled: (0, 13), Test: (703090, 41)\n",
      "Checking label distribution\n",
      "Training: 0.0979606313176095\n",
      "Validation: 0.09589052260946108\n",
      "Testing: 0.10476480792437651\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "seed = args.seed\n",
    "epochs = args.epoch\n",
    "dim = args.dim\n",
    "lr = args.lr\n",
    "weight_decay = args.l2\n",
    "initial_inspection_rate = args.initial_inspection_rate\n",
    "inspection_rate_option = args.inspection_plan\n",
    "mode = args.mode\n",
    "train_begin = args.train_from \n",
    "test_begin = args.test_from\n",
    "test_length = args.test_length\n",
    "valid_length = args.valid_length\n",
    "chosen_data = args.data\n",
    "numWeeks = args.numweeks\n",
    "semi_supervised = args.semi_supervised\n",
    "save = args.save\n",
    "gpu_id = args.device\n",
    "\n",
    "# Initial dataset split\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Initial dataset split\n",
    "train_start_day = datetime.date(int(train_begin[:4]), int(train_begin[4:6]), int(train_begin[6:8]))\n",
    "test_start_day = datetime.date(int(test_begin[:4]), int(test_begin[4:6]), int(test_begin[6:8]))\n",
    "test_length = timedelta(days=test_length)    \n",
    "test_end_day = test_start_day + test_length\n",
    "valid_length = timedelta(days=valid_length)\n",
    "valid_start_day = test_start_day - valid_length\n",
    "\n",
    "# data\n",
    "data.split(train_start_day, valid_start_day, test_start_day, test_end_day, valid_length, test_length, args)\n",
    "data.featureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes: 34454, Number of Edges: 30957\n",
      "Check node feature size: (34454, 29)\n"
     ]
    }
   ],
   "source": [
    "# re-label hscode\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data.dftrainx_lab[\"HS6\"])\n",
    "num_hs = len(encoder.classes_)\n",
    "\n",
    "# build transaction-hs bipartite graph\n",
    "G = nx.Graph()\n",
    "hs_nodes = encoder.transform(data.dftrainx_lab[\"HS6\"])\n",
    "transaction_nodes = np.array(range(data.dftrainx_lab.shape[0])) + num_hs\n",
    "labeled_nodes = np.array(range(data.dftrainx_lab.shape[0])) + num_hs\n",
    "train_edges = list(zip(hs_nodes,transaction_nodes))\n",
    "G.add_edges_from(train_edges)\n",
    "print(\"Number of Nodes: %d, Number of Edges: %d\" % (G.number_of_nodes(), G.number_of_edges()))\n",
    "\n",
    "# node feature\n",
    "scaler = MinMaxScaler()\n",
    "transaction_feature = scaler.fit_transform(data.dftrainx_lab.values)\n",
    "feature_dim = data.dftrainx_lab.shape[1]\n",
    "\n",
    "# init hs node embedding with zeros(only receive information from transaction)\n",
    "# refer to this paper https://arxiv.org/pdf/2011.12193.pdf\n",
    "nodeFeature = np.zeros((num_hs, feature_dim)) \n",
    "nodeFeature = np.vstack((nodeFeature,transaction_feature))\n",
    "print(\"Check node feature size:\",nodeFeature.shape)\n",
    "\n",
    "assert feature_dim == nodeFeature.shape[1]\n",
    "assert G.number_of_nodes() == nodeFeature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to PyG Dataloader format\n",
    "train_data = from_networkx(G)\n",
    "node_feature = torch.FloatTensor(nodeFeature)\n",
    "train_y = torch.FloatTensor(data.train_cls_label)\n",
    "train_data.x = node_feature\n",
    "train_data.y = train_y\n",
    "train_data.label_idx = torch.tensor(labeled_nodes) # record the transaction node index\n",
    "train_loader = DataLoader([train_data]*10,batch_size=1) # duplicate the graph for 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526/526 [06:06<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# construct validation data\n",
    "valid_batch_size = 256\n",
    "batch_start = 0\n",
    "validTranscation_feature = scaler.transform(data.dfvalidx_lab.values)\n",
    "data_list = []\n",
    "\n",
    "for batch_idx in trange(validTranscation_feature.shape[0] //valid_batch_size + 1):\n",
    "    # build graph\n",
    "    origin_nodeNum = G.number_of_nodes()\n",
    "    unseen_idx = G.number_of_nodes()\n",
    "    valid_hsnode = data.dfvalidx_lab[\"HS6\"][batch_start:batch_start+valid_batch_size].values\n",
    "    unseen_HS = 0\n",
    "    for i in range(len(valid_hsnode)):\n",
    "        if valid_hsnode[i] in encoder.classes_:\n",
    "            valid_hsnode[i] = encoder.transform([valid_hsnode[i]])[0]\n",
    "        else:\n",
    "            valid_hsnode[i] = unseen_idx\n",
    "            unseen_idx +=1\n",
    "            unseen_HS += 1\n",
    "    validTr_id = list(range(unseen_idx, unseen_idx+valid_batch_size))\n",
    "    valid_edges = list(zip(validTr_id,valid_hsnode))\n",
    "    G.add_edges_from(valid_edges)\n",
    "    \n",
    "    # node feautres\n",
    "    current_batch = validTranscation_feature[batch_start:batch_start+valid_batch_size,:]\n",
    "    current_feature = np.zeros((unseen_HS,feature_dim))\n",
    "    current_feature = torch.FloatTensor(np.vstack((current_feature, current_batch)))\n",
    "    current_feature = torch.cat((node_feature,current_feature), dim=0)\n",
    "    \n",
    "    # pyG data\n",
    "    valid_Data = Data()\n",
    "    valid_Data.edge_index = to_undirected(torch.LongTensor(list(G.edges())).T)\n",
    "    valid_Data.x = current_feature\n",
    "    valid_Data.y = torch.FloatTensor(data.valid_cls_label[batch_start:batch_start+valid_batch_size])\n",
    "    valid_Data.label_idx = torch.arange(origin_nodeNum + unseen_HS, origin_nodeNum + unseen_HS + valid_Data.y.shape[0])\n",
    "    G.remove_edges_from(valid_edges)\n",
    "    G.remove_nodes_from(list(range(origin_nodeNum,G.number_of_nodes())))\n",
    "    data_list.append(valid_Data)\n",
    "    batch_start+= valid_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(data_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchtools.optim import RangerLars\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self,inDim, outDim):\n",
    "        '''\n",
    "        Basic GNN model.\n",
    "        '''\n",
    "        super(GNN,self).__init__()\n",
    "        self.gnn = SAGEConv(inDim, outDim)\n",
    "        self.norm = nn.LayerNorm(outDim)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,edge_index):\n",
    "        feature = self.gnn(x,edge_index)\n",
    "        feature = self.act(self.norm(feature))\n",
    "        return feature\n",
    "\n",
    "class GNNStack(nn.Module):\n",
    "    def __init__(self,layer_dims):\n",
    "        '''\n",
    "        Create a model that stacks multi-layer GNN.\n",
    "        The embedding obtainable are concatenated as output. \n",
    "        '''\n",
    "        super(GNNStack,self).__init__()\n",
    "        self.gnns = nn.ModuleList([])\n",
    "        for i in range(len(layer_dims) -1):\n",
    "            gnn_module = GNN(layer_dims[i], layer_dims[i+1])\n",
    "            self.gnns.append(gnn_module)\n",
    "    \n",
    "    def forward(self,x,edge_index):\n",
    "        features = [x] # raw feature\n",
    "        current_feature = x\n",
    "        for gnn in self.gnns:\n",
    "            current_feature = gnn(current_feature,edge_index) # update node embedding\n",
    "            features.append(current_feature)\n",
    "    \n",
    "        return torch.cat(features,dim=-1) # concat node embedding of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_prob,xgb_testy,revenue_test, args, best_thresh=None, display=True):\n",
    "    \"\"\" Evaluate the performance\"\"\"\n",
    "    pr, re, f, rev = [], [], [], []\n",
    "    # For validatation, we measure the performance on 5% (previously, 1%, 2%, 5%, and 10%)\n",
    "    for i in [99,98,95,90]: \n",
    "        threshold = np.percentile(y_prob, i)\n",
    "        precision = xgb_testy[y_prob > threshold].mean()\n",
    "        recall = sum(xgb_testy[y_prob > threshold])/ sum(xgb_testy)\n",
    "        revenue = sum(revenue_test[y_prob > threshold]) / sum(revenue_test)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        if display:\n",
    "            print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "            print('Precision: %.4f, Recall: %.4f, Revenue: %.4f' % (precision, recall, revenue))\n",
    "        # save results\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "        f.append(f1)\n",
    "        rev.append(revenue)\n",
    "    return f, pr, re, rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLGNN(pl.LightningModule):\n",
    "    def __init__(self,layers):\n",
    "        super(SSLGNN,self).__init__()\n",
    "        self.gnn = GNNStack(layers) # GNN Layer\n",
    "        self.cls_layer = nn.Linear(sum(layers),1) # output layer\n",
    "        self.lr = 0.001\n",
    "        self.l2 = 0.0001\n",
    "        self.epochs = 200\n",
    "        self._weight_init()\n",
    "        \n",
    "        # hyperparameters\n",
    "        hparam = {\"lr\":self.lr, \"l2\":self.l2,\"epoch\":self.epochs}\n",
    "        self.hparams = hparam\n",
    "    \n",
    "    def _weight_init(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1 and p.requires_grad:\n",
    "                nn.init.kaiming_normal_(p)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x , data.edge_index\n",
    "        gnn_feature = self.gnn(x,edge_index)\n",
    "        out = torch.sigmoid(self.cls_layer(gnn_feature))\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        out = self(data)\n",
    "        out = out[data.label_idx,:] # This step selects the output of Transaction nodes\n",
    "        loss = F.binary_cross_entropy(out.flatten(), data.y.flatten())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self,batch, batch_idx):\n",
    "        data = batch\n",
    "        out = self(data)\n",
    "        out = out[data.label_idx,:]\n",
    "        loss = F.binary_cross_entropy(out.flatten(), data.y.flatten())\n",
    "        self.log(\"val_loss\",loss)\n",
    "        return out\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        y_prob = []\n",
    "        for pred in val_step_outputs:\n",
    "            prob = pred.detach().cpu().numpy().ravel().tolist()\n",
    "            y_prob.extend(prob)\n",
    "        y_prob = np.array(y_prob)\n",
    "        f,pr, re, rev = metrics(y_prob, data.valid_cls_label,data.valid_reg_label,args)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RangerLars(self.parameters(), lr=self.lr, weight_decay=self.l2)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type     | Params\n",
      "---------------------------------------\n",
      "0 | gnn       | GNNStack | 4.1 K \n",
      "1 | cls_layer | Linear   | 94    \n",
      "---------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192c805fffe3480297091cf55d3dc14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.1903, Recall: 0.0218, Revenue: 0.0239\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.2112, Recall: 0.0483, Revenue: 0.0431\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.1575, Recall: 0.0900, Revenue: 0.0775\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.1470, Recall: 0.1680, Revenue: 0.1308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.2632, Recall: 0.0301, Revenue: 0.0237\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.2405, Recall: 0.0550, Revenue: 0.0520\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.1946, Recall: 0.1112, Revenue: 0.0903\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.1768, Recall: 0.2020, Revenue: 0.1599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.3323, Recall: 0.0380, Revenue: 0.0351\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.3160, Recall: 0.0722, Revenue: 0.0693\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.2945, Recall: 0.1683, Revenue: 0.1519\n",
      "Checking top 10% suspicious transactions: 13445\n",
      "Precision: 0.2164, Recall: 0.2473, Revenue: 0.2085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.4981, Recall: 0.0569, Revenue: 0.0508\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.4349, Recall: 0.0994, Revenue: 0.0948\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3481, Recall: 0.1989, Revenue: 0.1883\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2622, Recall: 0.2996, Revenue: 0.2758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.5762, Recall: 0.0659, Revenue: 0.0566\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.4874, Recall: 0.1114, Revenue: 0.1113\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3650, Recall: 0.2086, Revenue: 0.2027\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2733, Recall: 0.3124, Revenue: 0.2967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.6000, Recall: 0.0686, Revenue: 0.0616\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.5019, Recall: 0.1147, Revenue: 0.1201\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3646, Recall: 0.2083, Revenue: 0.2065\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2836, Recall: 0.3241, Revenue: 0.3096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.6022, Recall: 0.0688, Revenue: 0.0600\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.5156, Recall: 0.1179, Revenue: 0.1169\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3643, Recall: 0.2082, Revenue: 0.2007\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2825, Recall: 0.3228, Revenue: 0.3127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.5896, Recall: 0.0674, Revenue: 0.0575\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.5164, Recall: 0.1181, Revenue: 0.1165\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3650, Recall: 0.2086, Revenue: 0.2002\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2826, Recall: 0.3230, Revenue: 0.3149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.5851, Recall: 0.0669, Revenue: 0.0580\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.5097, Recall: 0.1165, Revenue: 0.1094\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3653, Recall: 0.2088, Revenue: 0.2023\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2877, Recall: 0.3289, Revenue: 0.3227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.5903, Recall: 0.0675, Revenue: 0.0620\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.5037, Recall: 0.1152, Revenue: 0.1150\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3683, Recall: 0.2105, Revenue: 0.2112\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2912, Recall: 0.3328, Revenue: 0.3301\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1345\n",
      "Precision: 0.5866, Recall: 0.0671, Revenue: 0.0632\n",
      "Checking top 2% suspicious transactions: 2690\n",
      "Precision: 0.4892, Recall: 0.1119, Revenue: 0.1088\n",
      "Checking top 5% suspicious transactions: 6723\n",
      "Precision: 0.3626, Recall: 0.2072, Revenue: 0.2088\n",
      "Checking top 10% suspicious transactions: 13446\n",
      "Precision: 0.2915, Recall: 0.3332, Revenue: 0.3300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building model\n",
    "layers = [feature_dim,32,32] # 3-layer GNN\n",
    "model = SSLGNN(layers)\n",
    "\n",
    "# setting configs and logger for training (Can ignore this part)\n",
    "logger = TensorBoardLogger(\"ssl_exp\",name=\"SSL_GNN\")\n",
    "logger.log_hyperparams(model.hparams, metrics={\"F1-top\":0})\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='F1-top',\n",
    "    dirpath='./saved_model',\n",
    "    filename='SSLMLP-{epoch:02d}-{F1-top:.4f}',\n",
    "    save_top_k=0,\n",
    "    mode='max',\n",
    ")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "trainer = pl.Trainer(max_epochs=model.epochs,gpus=[0],\n",
    "#                      fast_dev_run=True,\n",
    "                     num_sanity_val_steps=0,\n",
    "                     check_val_every_n_epoch=10\n",
    "                    )\n",
    "\n",
    "# training\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
