{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from parser import get_parser\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.utils import from_networkx, to_undirected\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Ndata(path='../Custom-Semi-Supervised/data/ndata.csv')\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(args=\n",
    "                         [\"--data\",\"real-n\", \n",
    "                          \"--sampling\",\"xgb\",\n",
    "                          \"--train_from\",\"20140101\",\n",
    "                          \"--test_from\",\"20170101\",\n",
    "                          \"--test_length\",\"365\",\n",
    "                          \"--valid_length\",\"90\",\n",
    "                          \"--initial_inspection_rate\", \"5\",\n",
    "                          \"--final_inspection_rate\", \"10\",\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:\n",
      "Train labeled: (54134, 52), Train unlabeled: (1028538, 52), Valid labeled: (70917, 52), Valid unlabeled: (0, 26), Test: (274808, 52)\n",
      "Checking label distribution\n",
      "Training: 0.05022795615481618\n",
      "Validation: 0.035556788645191434\n",
      "Testing: 0.025360899366070794\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "seed = args.seed\n",
    "epochs = args.epoch\n",
    "dim = args.dim\n",
    "lr = args.lr\n",
    "weight_decay = args.l2\n",
    "initial_inspection_rate = args.initial_inspection_rate\n",
    "inspection_rate_option = args.inspection_plan\n",
    "train_begin = args.train_from \n",
    "test_begin = args.test_from\n",
    "test_length = args.test_length\n",
    "valid_length = args.valid_length\n",
    "chosen_data = args.data\n",
    "numWeeks = args.numweeks\n",
    "semi_supervised = args.semi_supervised\n",
    "save = args.save\n",
    "gpu_id = args.device\n",
    "\n",
    "# Initial dataset split\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Initial dataset split\n",
    "train_start_day = datetime.date(int(train_begin[:4]), int(train_begin[4:6]), int(train_begin[6:8]))\n",
    "test_start_day = datetime.date(int(test_begin[:4]), int(test_begin[4:6]), int(test_begin[6:8]))\n",
    "test_length = timedelta(days=test_length)    \n",
    "test_end_day = test_start_day + test_length\n",
    "valid_length = timedelta(days=valid_length)\n",
    "valid_start_day = test_start_day - valid_length\n",
    "\n",
    "# data\n",
    "data.split(train_start_day, valid_start_day, test_start_day, test_end_day, valid_length, test_length, args)\n",
    "data.featureEngineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 2749\n",
      "Precision: 0.1404, Recall: 0.0568, Revenue: 0.0942\n",
      "Checking top 2% suspicious transactions: 5497\n",
      "Precision: 0.0871, Recall: 0.0705, Revenue: 0.1269\n",
      "Checking top 5% suspicious transactions: 13741\n",
      "Precision: 0.0442, Recall: 0.0893, Revenue: 0.1859\n",
      "Checking top 10% suspicious transactions: 27481\n",
      "Precision: 0.0888, Recall: 0.3590, Revenue: 0.3713\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from utils import *\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=4, n_jobs=-1, eval_metric=\"error\", scale_pos_weight = 50)\n",
    "xgb.fit(data.dftrainx_lab,data.train_cls_label)\n",
    "\n",
    "best_thresh, best_auc = find_best_threshold(xgb,data.dfvalidx_lab, data.valid_cls_label)\n",
    "xgb_test_pred = xgb.predict_proba(data.dftestx)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.test_cls_label,data.test_reg_label,args,best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 271\n",
      "Precision: 0.8450, Recall: 0.3491, Revenue: 0.2835\n",
      "Checking top 2% suspicious transactions: 541\n",
      "Precision: 0.5508, Recall: 0.4543, Revenue: 0.3817\n",
      "Checking top 5% suspicious transactions: 1278\n",
      "Precision: 0.3192, Recall: 0.6220, Revenue: 0.5636\n",
      "Checking top 10% suspicious transactions: 2698\n",
      "Precision: 0.2124, Recall: 0.8735, Revenue: 0.8082\n"
     ]
    }
   ],
   "source": [
    "xgb_test_pred = xgb.predict_proba(data.dfvalidx_lab)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.valid_cls_label,data.valid_reg_label,args,best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 996\n",
      "Precision: 0.5452, Recall: 0.4772, Revenue: 0.4519\n",
      "Checking top 2% suspicious transactions: 1981\n",
      "Precision: 0.3448, Recall: 0.6002, Revenue: 0.5477\n",
      "Checking top 5% suspicious transactions: 4985\n",
      "Precision: 0.2050, Recall: 0.8981, Revenue: 0.8762\n",
      "Checking top 10% suspicious transactions: 9412\n",
      "Precision: 0.1139, Recall: 0.9420, Revenue: 0.9362\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=100, max_depth=4, n_jobs=-1, eval_metric=\"error\", scale_pos_weight = 2)\n",
    "xgb.fit(data.dftrainx_lab,data.train_cls_label)\n",
    "\n",
    "best_thresh, best_auc = find_best_threshold(xgb,data.dfvalidx_lab, data.valid_cls_label)\n",
    "xgb_test_pred = xgb.predict_proba(data.dftestx)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.test_cls_label,data.test_reg_label,args,best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(data.dftrainx_lab.columns,xgb.feature_importances_))).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.Series(dict(zip(data.dftrainx_lab.columns,xgb.feature_importances_))).sort_values()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "X_train_leaves = xgb.apply(data.dftrainx_lab).reshape(-1,100)\n",
    "X_valid_leaves = xgb.apply(data.dfvalidx_lab).reshape(-1,100)\n",
    "X_test_leaves = xgb.apply(data.dftestx).reshape(-1,100)\n",
    "\n",
    "# One-hot encoding for leaf index\n",
    "xgbenc = OneHotEncoder(categories=\"auto\")\n",
    "lr_trainx = xgbenc.fit_transform(X_train_leaves)\n",
    "lr_validx = xgbenc.transform(X_valid_leaves)\n",
    "lr_testx = xgbenc.transform(X_test_leaves)\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(lr_trainx, data.train_cls_label)\n",
    "\n",
    "best_thresh, best_auc = find_best_threshold(lr,lr_validx, data.valid_cls_label)\n",
    "xgb_test_pred = lr.predict_proba(lr_testx)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.test_cls_label,data.test_reg_label,args,best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchfm.model.dfm import DeepFactorizationMachineModel\n",
    "from torchfm.model.afm import AttentionalFactorizationMachineModel\n",
    "from torchfm.model.afn import AdaptiveFactorizationNetwork\n",
    "from torchfm.model.xdfm import ExtremeDeepFactorizationMachineModel\n",
    "from torchfm.layer import FeaturesEmbedding, FeaturesLinear, AttentionalFactorizationMachine\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchtools.optim import RangerLars\n",
    "from utils import process_leaf_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get leaf index from xgboost model \n",
    "X_train_leaves = xgb.apply(data.dftrainx_lab)\n",
    "X_trainunlab_leaves = xgb.apply(data.dftrainx_unlab)\n",
    "X_valid_leaves = xgb.apply(data.dfvalidx_lab)\n",
    "X_test_leaves = xgb.apply(data.dftestx)\n",
    "\n",
    "# One-hot encoding for leaf index\n",
    "X_leaves = np.concatenate((X_train_leaves, X_trainunlab_leaves, X_valid_leaves, X_test_leaves), axis=0)\n",
    "transformed_leaves, leaf_dim, new_leaf_index = process_leaf_idx(X_leaves)\n",
    "train_rows = X_train_leaves.shape[0]\n",
    "trainunlab_rows = X_trainunlab_leaves.shape[0] + train_rows\n",
    "valid_rows = X_valid_leaves.shape[0] + trainunlab_rows\n",
    "train_leaves, trainunlab_leaves, valid_leaves, test_leaves = transformed_leaves[:train_rows],\\\n",
    "                                  transformed_leaves[train_rows:trainunlab_rows],\\\n",
    "                                  transformed_leaves[trainunlab_rows:valid_rows],\\\n",
    "                                  transformed_leaves[valid_rows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_leaves, valid_leaves, test_leaves = torch.tensor(train_leaves), torch.tensor(valid_leaves), torch.tensor(test_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    def __init__(self,min_count = 5):\n",
    "        self.min_count = min_count\n",
    "        self.map = dict()\n",
    "        self.inv_map = dict()\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self,x):\n",
    "        self.is_fitted = True\n",
    "        if not isinstance(x,pd.Series):\n",
    "            x = pd.Series(x)\n",
    "        self.counts = x.value_counts()\n",
    "        valid_ser = self.counts[self.counts > self.min_count]\n",
    "        valid_category = list(valid_ser.index)\n",
    "        for i,v in enumerate(valid_category):\n",
    "            self.map[v] = i + 1\n",
    "            self.inv_map[i + 1] = v\n",
    "        self.nunique = i + 2\n",
    "            \n",
    "    def fit_transform(self,x):\n",
    "        self.fit(x)\n",
    "        result = [self.map.get(i,0) for i in x]\n",
    "        return result\n",
    "    \n",
    "    def transform(self,x):\n",
    "        if not self.is_fitted:\n",
    "            raise NotImplementedError\n",
    "        result = [self.map.get(i,0) for i in x]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fields = []\n",
    "train_fields = []\n",
    "valid_fields = []\n",
    "test_fields = []\n",
    "for col in data.profile_candidates:\n",
    "    df_col = data.train_lab[col]\n",
    "    val_col = data.valid_lab[col]\n",
    "    test_col = data.test[col]\n",
    "    encoder = LabelEncoder()\n",
    "    relabed = encoder.fit_transform(df_col)\n",
    "    valid_relabel = encoder.transform(val_col)\n",
    "    test_relabel = encoder.transform(test_col)\n",
    "    num_fields.append(encoder.nunique)\n",
    "    train_fields.append(relabed)\n",
    "    valid_fields.append(valid_relabel)\n",
    "    test_fields.append(test_relabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatDATE(nn.Module):\n",
    "    def __init__(self,field_dims,hidden_dim,attn_size, dropouts):\n",
    "        super().__init__()\n",
    "        self.leaf_embedding = nn.Embedding(leaf_dim,hidden_dim)\n",
    "        elf.num_fields = len(field_dims)\n",
    "        self.embedding = FeaturesEmbedding(field_dims, hidden_dim)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.afm = AttentionalFactorizationMachine(hidden_dim, attn_size, dropouts)\n",
    "        \n",
    "    def forward(self,x,leaf):\n",
    "        leaf_emb = self.leaf_embedding(x)\n",
    "        leaf_emb = torch.sum(leaf_emb,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fields = torch.tensor(train_fields).T\n",
    "valid_fields = torch.tensor(valid_fields).T\n",
    "test_fields = torch.tensor(test_fields).T\n",
    "tr_y = torch.FloatTensor(data.train_cls_label)\n",
    "val_y = torch.FloatTensor(data.valid_cls_label)\n",
    "test_y = torch.FloatTensor(data.test_cls_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_fields, tr_y)\n",
    "valid_dataset = TensorDataset(valid_fields, val_y)\n",
    "test_dataset = TensorDataset(test_fields, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()\n",
    "device = \"cuda:3\"\n",
    "model = DeepFactorizationMachineModel(num_fields,32,(32,16),0.2).to(device)\n",
    "# model = AttentionalFactorizationMachineModel(num_fields, 32,attn_size=4,dropouts=(0.2,0.2)).to(device)\n",
    "# optimizer = RangerLars(model.parameters(), lr = 0.05, weight_decay= 0.001)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, weight_decay= 0.001, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for step, (x,y) in enumerate(train_loader):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "#         if step % 20 ==0:\n",
    "#             print(\"Loss:%.4f\" % loss.item())\n",
    "                \n",
    "def test(loader):\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for step, (x,y) in enumerate(loader):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            preds.append(pred)\n",
    "    preds = torch.cat(preds).detach().cpu().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    train()\n",
    "    print(\"=\"*30+\"[Epoch %d]\"%(epoch+1) + \"=\"*30)\n",
    "    preds = test(test_loader)\n",
    "    res = torch_metrics(preds,data.test_cls_label,data.test_reg_label)\n",
    "#     preds = test(valid_loader)\n",
    "#     res = torch_metrics(preds,data.valid_cls_label,data.valid_reg_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test(test_loader)\n",
    "res = torch_metrics(preds,data.test_cls_label,data.test_reg_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking top 1% suspicious transactions: 998\n",
    "Precision: 0.1784, Recall: 0.1564, Revenue: 0.1629\n",
    "Checking top 2% suspicious transactions: 1996\n",
    "Precision: 0.1493, Recall: 0.2619, Revenue: 0.2424\n",
    "Checking top 5% suspicious transactions: 4990\n",
    "Precision: 0.1255, Recall: 0.5501, Revenue: 0.4845\n",
    "Checking top 10% suspicious transactions: 9980\n",
    "Precision: 0.0828, Recall: 0.7258, Revenue: 0.6765"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
