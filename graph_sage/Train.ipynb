{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from parser import get_parser\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.utils import from_networkx, to_undirected\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from xgboost import XGBClassifier\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.Ndata(path='../Custom-Semi-Supervised/data/ndata.csv')\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(args=\n",
    "                         [\"--data\",\"real-t\", \n",
    "                          \"--sampling\",\"xgb\",\n",
    "                          \"--mode\",\"scratch\",\n",
    "                          \"--train_from\",\"20140101\",\n",
    "                          \"--test_from\",\"20170101\",\n",
    "                          \"--test_length\",\"365\",\n",
    "                          \"--valid_length\",\"180\",\n",
    "                          \"--initial_inspection_rate\", \"3\",\n",
    "                          \"--final_inspection_rate\", \"10\",\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size:\n",
      "Train labeled: (30302, 51), Train unlabeled: (979778, 51), Valid labeled: (143509, 51), Valid unlabeled: (0, 26), Test: (274808, 51)\n",
      "Checking label distribution\n",
      "Training: 0.052372021949017154\n",
      "Validation: 0.03918984481922127\n",
      "Testing: 0.025360899366070794\n"
     ]
    }
   ],
   "source": [
    "# args\n",
    "seed = args.seed\n",
    "epochs = args.epoch\n",
    "dim = args.dim\n",
    "lr = args.lr\n",
    "weight_decay = args.l2\n",
    "initial_inspection_rate = args.initial_inspection_rate\n",
    "inspection_rate_option = args.inspection_plan\n",
    "mode = args.mode\n",
    "train_begin = args.train_from \n",
    "test_begin = args.test_from\n",
    "test_length = args.test_length\n",
    "valid_length = args.valid_length\n",
    "chosen_data = args.data\n",
    "numWeeks = args.numweeks\n",
    "semi_supervised = args.semi_supervised\n",
    "save = args.save\n",
    "gpu_id = args.device\n",
    "\n",
    "# Initial dataset split\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Initial dataset split\n",
    "train_start_day = datetime.date(int(train_begin[:4]), int(train_begin[4:6]), int(train_begin[6:8]))\n",
    "test_start_day = datetime.date(int(test_begin[:4]), int(test_begin[4:6]), int(test_begin[6:8]))\n",
    "test_length = timedelta(days=test_length)    \n",
    "test_end_day = test_start_day + test_length\n",
    "valid_length = timedelta(days=valid_length)\n",
    "valid_start_day = test_start_day - valid_length\n",
    "\n",
    "# data\n",
    "data.split(train_start_day, valid_start_day, test_start_day, test_end_day, valid_length, test_length, args)\n",
    "data.featureEngineering()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from pygData_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n"
     ]
    }
   ],
   "source": [
    "categories=[\"importer.id\",\"HS6\"]\n",
    "gdata = GraphData(data,use_xgb=True, categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1436\n",
      "Precision: 0.1950, Recall: 0.0517, Revenue: 0.1257\n",
      "Checking top 2% suspicious transactions: 2871\n",
      "Precision: 0.1358, Recall: 0.0721, Revenue: 0.1731\n",
      "Checking top 5% suspicious transactions: 7170\n",
      "Precision: 0.0734, Recall: 0.0972, Revenue: 0.2205\n",
      "Checking top 10% suspicious transactions: 14351\n",
      "Precision: 0.0471, Recall: 0.1249, Revenue: 0.2609\n",
      "--------------------------------------------------\n",
      "Checking top 1% suspicious transactions: 2749\n",
      "Precision: 0.1124, Recall: 0.0455, Revenue: 0.0782\n",
      "Checking top 2% suspicious transactions: 5497\n",
      "Precision: 0.0708, Recall: 0.0572, Revenue: 0.1080\n",
      "Checking top 5% suspicious transactions: 13741\n",
      "Precision: 0.0374, Recall: 0.0756, Revenue: 0.1660\n",
      "Checking top 10% suspicious transactions: 27478\n",
      "Precision: 0.1083, Recall: 0.4378, Revenue: 0.3609\n"
     ]
    }
   ],
   "source": [
    "best_thresh, best_auc = find_best_threshold(gdata.xgb,data.dfvalidx_lab, data.valid_cls_label)\n",
    "xgb_test_pred = gdata.xgb.predict_proba(data.dfvalidx_lab)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.valid_cls_label,data.valid_reg_label,best_thresh)\n",
    "print(\"-\"*50)\n",
    "xgb_test_pred = gdata.xgb.predict_proba(data.dftestx)[:,-1]\n",
    "overall_f1,auc,pr, re, f, rev = metrics(xgb_test_pred, data.test_cls_label,data.test_reg_label,best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"train_lab\"\n",
    "trainLab_data = gdata.get_data(stage)\n",
    "train_nodeidx = torch.tensor(gdata.get_AttNode(stage))\n",
    "trainLab_data.node_idx = train_nodeidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"train_unlab\"\n",
    "unlab_data = gdata.get_data(stage)\n",
    "unlab_nodeidx = torch.tensor(gdata.get_AttNode(stage))\n",
    "unlab_data.node_idx = unlab_nodeidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"valid\"\n",
    "valid_data = gdata.get_data(stage)\n",
    "valid_nodeidx = torch.tensor(gdata.get_AttNode(stage))\n",
    "valid_data.node_idx = valid_nodeidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage = \"test\"\n",
    "test_data = gdata.get_data(stage)\n",
    "test_nodeidx = torch.tensor(gdata.get_AttNode(stage))\n",
    "test_data.node_idx = test_nodeidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[60604], edge_index=[2, 121208], edge_label=[121208], node_idx=[30302], rev=[45573], x=[45573, 100], y=[45573])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLab_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[287018], edge_index=[2, 574036], edge_label=[574036], node_idx=[143509], rev=[153663], x=[153663, 100], y=[153663])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[549616], edge_index=[2, 1099232], edge_label=[1099232], node_idx=[274808], rev=[292483], x=[292483, 100], y=[292483])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchtools.optim import RangerLars\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLGNN(LightningModule):\n",
    "    def __init__(self, data, input_dim, hidden_dim, numLayers, useXGB=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.data = data\n",
    "        self.input_dim = input_dim\n",
    "        self.dim = hidden_dim*2\n",
    "        self.numLayers = numLayers\n",
    "        self.layers = [self.dim, self.dim//2] #* (numLayers+1)\n",
    "        self.bn = nn.BatchNorm1d(self.dim)\n",
    "        self.act = Mish()\n",
    "        self.useXGB = useXGB\n",
    "        \n",
    "        # GNN layer\n",
    "        if self.useXGB:\n",
    "            self.initEmbedding = nn.Embedding(self.input_dim, self.dim, padding_idx=0)\n",
    "        else:\n",
    "            self.initEmbedding = MLP(self.input_dim, self.dim, Numlayer=2)\n",
    "        self.initGNN = UselessConv()\n",
    "        self.GNNs = GNNStack(self.layers,self.numLayers)\n",
    "        \n",
    "        # output\n",
    "        self.outLayer = nn.ModuleList([nn.Linear(self.dim,1) for _ in range(numLayers+1)])\n",
    "        self.revLayer = nn.ModuleList([nn.Linear(self.dim,1) for _ in range(numLayers+1)])\n",
    "        self.combined = nn.Linear(numLayers+1, 1, bias=False)\n",
    "        self.combinedRev = nn.Linear(numLayers+1, 1, bias=False)\n",
    "        self.loss_func = FocalLoss(logits=True)\n",
    "\n",
    "    def forward(self, x,adjs):\n",
    "        # update node embedding\n",
    "        leaf_emb = self.initEmbedding(x)\n",
    "        if self.useXGB:\n",
    "            leaf_emb = torch.sum(leaf_emb,dim=1) # summation over the trees\n",
    "            leaf_emb = self.bn(leaf_emb)\n",
    "            leaf_emb = self.act(leaf_emb)\n",
    "        \n",
    "        # first update \n",
    "        firstHop_neighbor = adjs[-2][0]\n",
    "        leaf_emb = self.initGNN(leaf_emb,to_undirected(firstHop_neighbor))\n",
    "        \n",
    "        # GNN \n",
    "        embeddings = self.GNNs(leaf_emb, adjs)\n",
    "        \n",
    "        # logits\n",
    "        logits = [self.outLayer[i](v) for i,v in enumerate(embeddings)]\n",
    "        ensemble = torch.cat(logits, dim=-1)\n",
    "        ensemble = self.combined(ensemble)\n",
    "        logits.append(ensemble)\n",
    "        \n",
    "        # revenue\n",
    "        revenues = [torch.relu(self.revLayer[i](v)) for i,v in enumerate(embeddings)]\n",
    "        ensemble = torch.cat(revenues, dim=-1)\n",
    "        ensemble = self.combinedRev(ensemble)\n",
    "        revenues.append(ensemble)\n",
    "        \n",
    "        return logits, revenues\n",
    "    \n",
    "    def compute_CLS_loss(self,Logits, label):\n",
    "        loss = 0\n",
    "        for logit in Logits:\n",
    "            logit = logit.flatten()\n",
    "            l = self.loss_func(logit,label)\n",
    "            loss+= l\n",
    "        return loss\n",
    "    \n",
    "    def compute_REG_loss(self,preds, rev):\n",
    "        loss = 0\n",
    "        for pred in preds:\n",
    "            pred = pred.flatten()\n",
    "            l = F.mse_loss(pred,rev)\n",
    "            loss += l\n",
    "        return loss \n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        target_idx = torch.arange(batch.y.shape[0])\n",
    "        logits, revenues = self(batch.x, batch.adjs_t)\n",
    "        CLS_loss = self.compute_CLS_loss(logits, batch.y)  \n",
    "        REG_loss = self.compute_REG_loss(revenues, batch.rev)\n",
    "        train_loss = CLS_loss + 10 * REG_loss\n",
    "        self.log('train_loss', train_loss)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int):\n",
    "        target_idx = torch.arange(batch.y.shape[0])\n",
    "        logits, revenues = self(batch.x, batch.adjs_t)\n",
    "        CLS_loss = self.compute_CLS_loss(logits, batch.y)  \n",
    "        REG_loss = self.compute_REG_loss(revenues, batch.rev)\n",
    "        valid_loss = CLS_loss + 1 * REG_loss\n",
    "        self.log('val_loss', valid_loss, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return logits[-1]\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        predictions = torch.cat(val_step_outputs).detach().cpu().numpy().ravel()\n",
    "        f,pr, re, rev = torch_metrics(predictions, self.data.valid_cls_label, self.data.valid_reg_label)\n",
    "        f1_top = np.mean(f)\n",
    "        self.log(\"F1-top\",f1_top)\n",
    "        performance = [*f, *pr, *re, *rev]\n",
    "        name_performance = [\"F1@1\",\"F1@2\",\"F1@5\",\"F1@10\",\"Pr@1\",\"Pr@2\",\"Pr@5\",\"Pr@10\",\n",
    "                            \"Re@1\",\"Re@2\",\"Re@5\",\"Re@10\",\"Rev@1\",\"Rev@2\",\"Rev@5\",\"Rev@10\"]\n",
    "        name_performance = [\"Val/\"+i for i in name_performance]\n",
    "        tensorboard_logs = dict(zip(name_performance,performance))\n",
    "        return {\"Val/F1-top\":f1_top, \"log\":tensorboard_logs}\n",
    "        \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "    \n",
    "    def test_epoch_end(self,val_step_outputs):\n",
    "        predictions = torch.cat(val_step_outputs).detach().cpu().numpy().ravel()\n",
    "        f,pr, re, rev = torch_metrics(predictions, self.data.test_cls_label, self.data.test_reg_label)\n",
    "        f1_top = np.mean(f)\n",
    "        performance = [*f, *pr, *re, *rev]\n",
    "        name_performance = [\"F1@1\",\"F1@2\",\"F1@5\",\"F1@10\",\"Pr@1\",\"Pr@2\",\"Pr@5\",\"Pr@10\",\n",
    "                            \"Re@1\",\"Re@2\",\"Re@5\",\"Re@10\",\"Rev@1\",\"Rev@2\",\"Rev@5\",\"Rev@10\"]\n",
    "        name_performance = [\"Test/\"+i for i in name_performance]\n",
    "        tensorboard_logs = dict(zip(name_performance,performance))\n",
    "        \n",
    "        return {\"F1-top\":f1_top, \"log\":tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RangerLars(self.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLGNN(LightningModule):\n",
    "    def __init__(self,input_dim, hidden_dim, numLayers, useXGB=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.input_dim = input_dim\n",
    "        self.dim = hidden_dim*2\n",
    "        self.numLayers = numLayers\n",
    "        self.layers = [self.dim, self.dim//2] #* (numLayers+1)\n",
    "        self.bn = nn.BatchNorm1d(self.dim)\n",
    "        self.act = Mish()\n",
    "        self.useXGB = useXGB\n",
    "        \n",
    "        # GNN layer\n",
    "        if self.useXGB:\n",
    "            self.initEmbedding = nn.Embedding(self.input_dim, self.dim, padding_idx=0)\n",
    "        else:\n",
    "            self.initEmbedding = MLP(self.input_dim, self.dim, Numlayer=2)\n",
    "        self.initGNN = UselessConv()\n",
    "        self.GNNs = GNNStack(self.layers,self.numLayers)\n",
    "        \n",
    "        # output\n",
    "        self.outLayer = nn.ModuleList([nn.Linear(self.dim,1) for _ in range(numLayers+1)])\n",
    "        self.revLayer = nn.ModuleList([nn.Linear(self.dim,1) for _ in range(numLayers+1)])\n",
    "        self.combined = nn.Linear(numLayers+1, 1, bias=False)\n",
    "        self.combinedRev = nn.Linear(numLayers+1, 1, bias=False)\n",
    "        self.loss_func = FocalLoss(logits=True)\n",
    "\n",
    "    def forward(self, x,adjs):\n",
    "        # update node embedding\n",
    "        leaf_emb = self.initEmbedding(x)\n",
    "        if self.useXGB:\n",
    "            leaf_emb = torch.sum(leaf_emb,dim=1) # summation over the trees\n",
    "            leaf_emb = self.bn(leaf_emb)\n",
    "            leaf_emb = self.act(leaf_emb)\n",
    "        \n",
    "        # first update \n",
    "        firstHop_neighbor = adjs[-2][0]\n",
    "        leaf_emb = self.initGNN(leaf_emb,to_undirected(firstHop_neighbor))\n",
    "        \n",
    "        # GNN \n",
    "        embeddings = self.GNNs(leaf_emb, adjs)\n",
    "        \n",
    "        # logits\n",
    "        logits = [self.outLayer[i](v) for i,v in enumerate(embeddings)]\n",
    "        ensemble = torch.cat(logits, dim=-1)\n",
    "        ensemble = self.combined(ensemble)\n",
    "        logits.append(ensemble)\n",
    "        \n",
    "        # revenue\n",
    "        revenues = [torch.relu(self.revLayer[i](v)) for i,v in enumerate(embeddings)]\n",
    "        ensemble = torch.cat(revenues, dim=-1)\n",
    "        ensemble = self.combinedRev(ensemble)\n",
    "        revenues.append(ensemble)\n",
    "        \n",
    "        return logits, revenues\n",
    "    \n",
    "    def compute_CLS_loss(self,Logits, label):\n",
    "        loss = 0\n",
    "        for logit in Logits:\n",
    "            logit = logit.flatten()\n",
    "            l = self.loss_func(logit,label)\n",
    "            loss+= l\n",
    "        return loss\n",
    "    \n",
    "    def compute_REG_loss(self,preds, rev):\n",
    "        loss = 0\n",
    "        for pred in preds:\n",
    "            pred = pred.flatten()\n",
    "            l = F.mse_loss(pred,rev)\n",
    "            loss += l\n",
    "        return loss \n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        target_idx = torch.arange(batch.y.shape[0])\n",
    "        logits, revenues = self(batch.x, batch.adjs_t)\n",
    "        CLS_loss = self.compute_CLS_loss(logits, batch.y)  \n",
    "        REG_loss = self.compute_REG_loss(revenues, batch.rev)\n",
    "        train_loss = CLS_loss + 10 * REG_loss\n",
    "        self.log('train_loss', train_loss)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int):\n",
    "        target_idx = torch.arange(batch.y.shape[0])\n",
    "        logits, revenues = self(batch.x, batch.adjs_t)\n",
    "        CLS_loss = self.compute_CLS_loss(logits, batch.y)  \n",
    "        REG_loss = self.compute_REG_loss(revenues, batch.rev)\n",
    "        valid_loss = CLS_loss + 1 * REG_loss\n",
    "        self.log('val_loss', valid_loss, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return logits[-1]\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        predictions = torch.cat(val_step_outputs).detach().cpu().numpy().ravel()\n",
    "        f,pr, re, rev = torch_metrics(predictions, self.data.valid_cls_label, self.data.valid_reg_label)\n",
    "        f1_top = np.mean(f)\n",
    "        self.log(\"F1-top\",f1_top)\n",
    "        performance = [*f, *pr, *re, *rev]\n",
    "        name_performance = [\"F1@1\",\"F1@2\",\"F1@5\",\"F1@10\",\"Pr@1\",\"Pr@2\",\"Pr@5\",\"Pr@10\",\n",
    "                            \"Re@1\",\"Re@2\",\"Re@5\",\"Re@10\",\"Rev@1\",\"Rev@2\",\"Rev@5\",\"Rev@10\"]\n",
    "        name_performance = [\"Val/\"+i for i in name_performance]\n",
    "        tensorboard_logs = dict(zip(name_performance,performance))\n",
    "        return {\"Val/F1-top\":f1_top, \"log\":tensorboard_logs}\n",
    "        \n",
    "    def test_step(self,batch, batch_idx):\n",
    "        return self.validation_step(batch, batch_idx)\n",
    "    \n",
    "    def test_epoch_end(self,val_step_outputs):\n",
    "        predictions = torch.cat(val_step_outputs).detach().cpu().numpy().ravel()\n",
    "        f,pr, re, rev = torch_metrics(predictions, self.data.test_cls_label, self.data.test_reg_label)\n",
    "        f1_top = np.mean(f)\n",
    "        performance = [*f, *pr, *re, *rev]\n",
    "        name_performance = [\"F1@1\",\"F1@2\",\"F1@5\",\"F1@10\",\"Pr@1\",\"Pr@2\",\"Pr@5\",\"Pr@10\",\n",
    "                            \"Re@1\",\"Re@2\",\"Re@5\",\"Re@10\",\"Rev@1\",\"Rev@2\",\"Rev@5\",\"Rev@10\"]\n",
    "        name_performance = [\"Test/\"+i for i in name_performance]\n",
    "        tensorboard_logs = dict(zip(name_performance,performance))\n",
    "        \n",
    "        return {\"F1-top\":f1_top, \"log\":tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = RangerLars(self.parameters(), lr=0.05, weight_decay=0.0001)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.99)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name          | Type        | Params\n",
      "----------------------------------------------\n",
      "0 | bn            | BatchNorm1d | 128   \n",
      "1 | act           | Mish        | 0     \n",
      "2 | initEmbedding | Embedding   | 74.0 K\n",
      "3 | initGNN       | UselessConv | 0     \n",
      "4 | GNNs          | GNNStack    | 33.5 K\n",
      "5 | outLayer      | ModuleList  | 195   \n",
      "6 | revLayer      | ModuleList  | 195   \n",
      "7 | combined      | Linear      | 3     \n",
      "8 | combinedRev   | Linear      | 3     \n",
      "9 | loss_func     | FocalLoss   | 0     \n",
      "----------------------------------------------\n",
      "108 K     Trainable params\n",
      "0         Non-trainable params\n",
      "108 K     Total params\n",
      "INFO:lightning:\n",
      "  | Name          | Type        | Params\n",
      "----------------------------------------------\n",
      "0 | bn            | BatchNorm1d | 128   \n",
      "1 | act           | Mish        | 0     \n",
      "2 | initEmbedding | Embedding   | 74.0 K\n",
      "3 | initGNN       | UselessConv | 0     \n",
      "4 | GNNs          | GNNStack    | 33.5 K\n",
      "5 | outLayer      | ModuleList  | 195   \n",
      "6 | revLayer      | ModuleList  | 195   \n",
      "7 | combined      | Linear      | 3     \n",
      "8 | combinedRev   | Linear      | 3     \n",
      "9 | loss_func     | FocalLoss   | 0     \n",
      "----------------------------------------------\n",
      "108 K     Trainable params\n",
      "0         Non-trainable params\n",
      "108 K     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3fb5033a214df3b4428593b2316075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1436\n",
      "Precision: 0.0815, Recall: 0.0216, Revenue: 0.0146\n",
      "Checking top 2% suspicious transactions: 2871\n",
      "Precision: 0.0662, Recall: 0.0351, Revenue: 0.0215\n",
      "Checking top 5% suspicious transactions: 7175\n",
      "Precision: 0.0461, Recall: 0.0612, Revenue: 0.1087\n",
      "Checking top 10% suspicious transactions: 14351\n",
      "Precision: 0.0427, Recall: 0.1133, Revenue: 0.2013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1436\n",
      "Precision: 0.0843, Recall: 0.0224, Revenue: 0.0387\n",
      "Checking top 2% suspicious transactions: 2871\n",
      "Precision: 0.0711, Recall: 0.0377, Revenue: 0.0851\n",
      "Checking top 5% suspicious transactions: 7176\n",
      "Precision: 0.0492, Recall: 0.0652, Revenue: 0.1267\n",
      "Checking top 10% suspicious transactions: 14351\n",
      "Precision: 0.0415, Recall: 0.1099, Revenue: 0.2488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 1436\n",
      "Precision: 0.0884, Recall: 0.0235, Revenue: 0.0405\n",
      "Checking top 2% suspicious transactions: 2871\n",
      "Precision: 0.0714, Recall: 0.0379, Revenue: 0.0853\n",
      "Checking top 5% suspicious transactions: 7176\n",
      "Precision: 0.0497, Recall: 0.0660, Revenue: 0.1371\n",
      "Checking top 10% suspicious transactions: 14351\n",
      "Precision: 0.0419, Recall: 0.1112, Revenue: 0.2414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa4d76a99e448dda08eb36ad334aa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model config\n",
    "seed_everything(1234)\n",
    "input_dim = gdata.leaf_dim\n",
    "hidden_size = 32\n",
    "sizes = [-1,200]\n",
    "numLayers = len(sizes)\n",
    "batch_size = 512\n",
    "\n",
    "model = SSLGNN(input_dim, hidden_size, numLayers, useXGB=gdata.use_xgb)\n",
    "model.data = data\n",
    "\n",
    "# lightning config\n",
    "stacked_data = StackData(trainLab_data,unlab_data,valid_data, test_data)\n",
    "datamodule = CustomData(stacked_data, sizes = sizes, batch_size=batch_size)\n",
    "logger = TensorBoardLogger(\"ssl_exp\",name=\"SSL_GNN\")\n",
    "logger.log_hyperparams(model.hparams, metrics={\"F1-top\":0})\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='F1-top',    \n",
    "    dirpath='./saved_model',\n",
    "    filename='GNN-{epoch:02d}-{F1-top:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='max',\n",
    ")\n",
    "trainer = Trainer(gpus=[3], max_epochs=20,\n",
    "                  logger = logger,\n",
    "                 num_sanity_val_steps=0,\n",
    "                  check_val_every_n_epoch=1,\n",
    "                  callbacks=[checkpoint_callback],\n",
    "#                   fast_dev_run=True\n",
    "                 )\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking top 1% suspicious transactions: 2749<br>\n",
    "Precision: 0.1124, Recall: 0.0455, Revenue: 0.0782<br>\n",
    "Checking top 2% suspicious transactions: 5497<br>\n",
    "Precision: 0.0708, Recall: 0.0572, Revenue: 0.1080<br>\n",
    "Checking top 5% suspicious transactions: 13741<br>\n",
    "Precision: 0.0374, Recall: 0.0756, Revenue: 0.1660<br>\n",
    "Checking top 10% suspicious transactions: 27478<br>\n",
    "Precision: 0.1083, Recall: 0.4378, Revenue: 0.3609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dhome/roytsai/gnn_wco/saved_model/GNN-epoch=15-F1-top=0.0991.ckpt'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc3e13e72d04f38a69179063a86c43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking top 1% suspicious transactions: 2749\n",
      "Precision: 0.1131, Recall: 0.0458, Revenue: 0.0792\n",
      "Checking top 2% suspicious transactions: 5497\n",
      "Precision: 0.0737, Recall: 0.0596, Revenue: 0.1121\n",
      "Checking top 5% suspicious transactions: 13741\n",
      "Precision: 0.0595, Recall: 0.1202, Revenue: 0.1867\n",
      "Checking top 10% suspicious transactions: 27481\n",
      "Precision: 0.0785, Recall: 0.3175, Revenue: 0.2862\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'F1-top': 0.08412887078527967,\n",
      " 'Test/F1@1': 0.0651581814372512,\n",
      " 'Test/F1@10': 0.12591166345761126,\n",
      " 'Test/F1@2': 0.06588579795021962,\n",
      " 'Test/F1@5': 0.0795598402960366,\n",
      " 'Test/Pr@1': 0.11313204801746089,\n",
      " 'Test/Pr@10': 0.07852698227866525,\n",
      " 'Test/Pr@2': 0.07367655084591596,\n",
      " 'Test/Pr@5': 0.05945709919219853,\n",
      " 'Test/Re@1': 0.0457554803589819,\n",
      " 'Test/Re@10': 0.31749301162277477,\n",
      " 'Test/Re@2': 0.05958511107841695,\n",
      " 'Test/Re@5': 0.12020008827423863,\n",
      " 'Test/Rev@1': 0.0791594356297533,\n",
      " 'Test/Rev@10': 0.2862099433718457,\n",
      " 'Test/Rev@2': 0.112061750658259,\n",
      " 'Test/Rev@5': 0.18668592310287727,\n",
      " 'Val/F1-top': 0.09302494388654133,\n",
      " 'Val/F1@1': 0.08615654205607477,\n",
      " 'Val/F1@10': 0.10656276881040329,\n",
      " 'Val/F1@2': 0.0919956537486418,\n",
      " 'Val/F1@5': 0.08738481093104544,\n",
      " 'Val/Pr@1': 0.20543175487465182,\n",
      " 'Val/Pr@10': 0.07337467772280677,\n",
      " 'Val/Pr@2': 0.13270637408568442,\n",
      " 'Val/Pr@5': 0.07664437012263099,\n",
      " 'Val/Re@1': 0.05450849963045085,\n",
      " 'Val/Re@10': 0.19456762749445677,\n",
      " 'Val/Re@2': 0.07039911308203992,\n",
      " 'Val/Re@5': 0.1016260162601626,\n",
      " 'Val/Rev@1': 0.15892678111477365,\n",
      " 'Val/Rev@10': 0.28651064658722397,\n",
      " 'Val/Rev@2': 0.1819338440696912,\n",
      " 'Val/Rev@5': 0.21894756084645664,\n",
      " 'val_loss': tensor(0.4471, device='cuda:3'),\n",
      " 'val_loss_epoch': tensor(0.6051, device='cuda:3')}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "testing_summary = trainer.test(model, test_dataloaders=datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform summary as dataframe for saving csv\n",
    "# df_summary = pd.DataFrame(testing_summary)\n",
    "# df_summary = df_summary[[i for i in df_summary.columns if \"Test\" in i]]\n",
    "# df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
